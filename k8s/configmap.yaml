apiVersion: v1
data:
  flink-conf.yaml: |
    blob.server.port: 6124
    kubernetes.jobmanager.annotations: flinkdeployment.flink.apache.org/generation:2
    kubernetes.jobmanager.replicas: 1
    execution.submit-failed-job-on-application-error: true
    jobmanager.rpc.address: stage-pyflink-job.flink-ops
    state.savepoints.dir: s3://eks-flink-bucket/savepoints
    kubernetes.service-account: flink
    kubernetes.cluster-id: stage-pyflink-job
    kubernetes.taskmanager.cpu.amount: 1.0
    $internal.application.program-args: -py;/opt/pyflink-parser-job/main.py
    parallelism.default: 4
    kubernetes.namespace: flink-ops
    taskmanager.numberOfTaskSlots: 4
    kubernetes.rest-service.exposed.type: ClusterIP
    kubernetes.jobmanager.owner.reference: uid:1c7b0c5f-c5c2-41d8-84c5-beec1db4de66,name:stage-pyflink-job,controller:false,blockOwnerDeletion:true,apiVersion:flink.apache.org/v1beta1,kind:FlinkDeployment
    kubernetes.container.image.ref: 392698013629.dkr.ecr.us-east-1.amazonaws.com/se-flink-job-stage-eks:v1
    execution.checkpointing.mode: EXACTLY_ONCE
    taskmanager.memory.process.size: 2 gb
    kubernetes.internal.jobmanager.entrypoint.class: org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint
    pipeline.name: stage-pyflink-job
    web.cancel.enable: false
    execution.target: kubernetes-application
    jobmanager.memory.process.size: 2 gb
    execution.shutdown-on-application-finish: false
    taskmanager.rpc.port: 6122
    execution.checkpointing.interval: 30s
    execution.checkpointing.timeout: 10min
    kubernetes.jobmanager.cpu.amount: 1.0
    internal.cluster.execution-mode: NORMAL
    env.java.default-opts.all: --add-exports=java.base/sun.net.util=ALL-UNNAMED --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
    $internal.pipeline.job-id: 99582858ad97d37f0a5fe0e961135b9a
    pipeline.jars: local:///opt/flink/lib/python-container.jar
    execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
    $internal.flink.version: v1_20
    kubernetes.pod-template-file.jobmanager: /tmp/flink_op_generated_podTemplate_5467164869350472562.yaml
    state.checkpoints.dir: s3://eks-flink-bucket/checkpoints
  log4j-console.properties: |
    ################################################################################
    #  Licensed to the Apache Software Foundation (ASF) under one
    #  or more contributor license agreements.  See the NOTICE file
    #  distributed with this work for additional information
    #  regarding copyright ownership.  The ASF licenses this file
    #  to you under the Apache License, Version 2.0 (the
    #  "License"); you may not use this file except in compliance
    #  with the License.  You may obtain a copy of the License at
    #
    #      http://www.apache.org/licenses/LICENSE-2.0
    #
    #  Unless required by applicable law or agreed to in writing, software
    #  distributed under the License is distributed on an "AS IS" BASIS,
    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #  See the License for the specific language governing permissions and
    # limitations under the License.
    ################################################################################

    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF

    # The monitor interval in seconds to enable log4j automatic reconfiguration
    # monitorInterval = 30
    # Flink Deployment Logging Overrides
    # rootLogger.level = DEBUG
kind: ConfigMap
metadata:
  labels:
    app: stage-pyflink-job
    type: flink-native-kubernetes
  name: flink-config-stage-pyflink-job
  namespace: flink-ops
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: stage-pyflink-job
    
